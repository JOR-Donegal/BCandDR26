{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>Business Continuity and Disaster Recovery</p> <p>Modern disk systems can be made very reliable. It is assumed you have previously covered RAID, clustering, volume copy, mirroring, distributed file systems, encoding, erasure coding. Single points of failure are eliminated in modern storage systems. A common fallacy is to assume that resilience and high availability is the same thing as a backup; it is not. </p> <ol> <li>People make mistakes and inadvertently destroy storage systems.</li> <li>Malicious actors may do the same.</li> <li>Ransomware has been a very significant cause of data loss; other forms of malware may also be.</li> <li>Physical incidents may occur; fire and floods are common in Ireland, other forms of disasters in other countries.</li> <li>Grid failures combined with the failure of backup power may cause power loss events.</li> <li>Physical equipment failure in a data centre may cause catastrophic damage.</li> </ol> <p>Every business should have a clear data retention policy which is compliant with legislative and governance requirements including GDPR, backups must be compliant with this policy. Things can go badly wrong if this is not the case.   </p>"},{"location":"a/","title":"Terminology","text":"<p>Before we start, terminology again!</p> <p>Storage is where we are keeping data. - For domestic use, a workstations disks.  - On a server, a storage array.  - In a data center, a dedicated storage device, like Network Attached Storage (NAS) or a Storage Area Network (SAN).</p> <p>A backup is the process of creating duplicate copies of data retained solely to allow for its recovery should it be lost, deleted, or corrupted.  Live synchronizing to a cloud store is not a backup!  The technology required for backups is dictated by the volume of data, the time available to backup, the stability/dynamics of the data, versioning of data, restore frequency characteristics, etc.  In the 1990s, we could back up an organization's critical data of a tape. By the 2000s, we were using tape libraries. In this decade, backup is often to disk and then tape, or to disk and then cloud. The issues in using these solutions are complex.  The simplest layer of protection in a business are the backups.</p> <p>Business Continuity (BC) is the ability of an organization to continue to operate in the event of a disruption. It involves plans, equipment and people. If a small fire occurs in a server room, what are the services that were provided from that room, and how do we put them operational again? Where is the data? Where do we get equipment to put those services back in operation? Where are the instructions to do all of this and who has been trained to do it? This is Business Continuity Planning (BCP) and we can expect to need it periodically in any business.</p> <p>A Disaster is a more major event, Disaster Recovery Planning (DRP) are the procedures to respond to such an event. </p> <p>Risk Analysis is a complex process, it is is the process of </p> <ul> <li>Identifying potential threats</li> <li>Assess how likely they are to occur (L)</li> <li>Evaluate the impact (I)</li> <li>Prioritize on this basis (L*I)</li> <li>Identify mitigation strategies in order of priority</li> </ul> <p>This is done badly in most instances I have seen. Qualitative Risk Analysis will use arbitrary judgement and categories (Low, Medium, High) to produce a model.</p> <p>In specialist domains such as the automobile or aviation industry, more formal methods may be used. Quantitative Risk Analysis uses numerical data to measure risk and potential losses.</p>"},{"location":"b/","title":"Principles","text":"<p>Some analysis needs to be carried out to understand the characteristics of the most typical restores requested and the reason for them. Tapes may be too slow to use as a primary backup mechanism. On many sites we now do backup to disk during the backup window and then backup to tape at our leisure; this process is known as staging the backups. Other techniques exist to make this process efficient; volume copy, replication, cloning etc. Any system planning for backup should meet these typical requirements.</p> <p>The Recovery-Point Objective (RPO) is a point in time at which data should be recoverable after an incident. This defines the granularity of backups and the customer\u2019s ability to tolerate data loss. If we backup at 2200 every evening, are we tolerating a full business day of potential data loss? If we want zero RPO, we need to mirror each change to a backup system which has versioning. For example, many Cloud solutions will back up in real-time and keep up to 30 copies of a document in a revision archive.</p> <p>The Recovery Time Objective (RTO) is the time it takes to reinstate data after an incident. It defines the customer\u2019s ability to tolerate downtime or the unavailability of systems or data. RTO determines the kind of backup systems and media we can use. Where the RTO is in days, we can restore from off-site backup tapes or to a cold recovery site. To ensure an RTO of less than a day, we would need a hot recovery site.</p> <p>To enable recovery in hours, we need to be able to recover from disk, not tape. There are exceptions, where complex tape robot systems are employed.</p> <p>For shorter term recovery times, we are leaving the realm of pure backup/restore.</p> <p>If I am spec\u2019ing a backup system for an SME, I insist that the backups are formally checked, typically once a week. I had a site who asked me to recover their data from tape after a server failure, they had no IT contractor. When I examined the tape backup, I realized that the customer had the same tape in the drive for six months and it had worn out at some point. Data recovery\u2026. six months out of date! The customer agreed to a proper maintenance contract after that, but too late for their data. I have far too many examples of that, where small businesses have had a a100% data loss. During the recent spate of ransomware infections, almost every business that contacted me after the event had a 100% data loss.</p> <p>Backups are essential and so is the quality assurance, knowing that the backups work. In an SME, I specify that a test restore needs to be done once a week. It is the only way to be sure the backups are working.</p> <p>With larger sites, the principle is the same, but the frequency and detail of checks will be different. In any DR exercise these processes will be procedural, complex, and thorough. The QA testing is a very effective way of ensuring that data recovery can take place, it is a good exercise to prepare for a real incident of potential data loss. Gaps are identified and mitigated, and the recovery team become familiar with the processes. </p> <p>In a data archive, durability refers to the ability of stored data to remain intact, uncorrupted, and permanently preserved over long periods of time, even in the face of hardware failures, software errors, or environmental issues.</p>"},{"location":"c/","title":"Backup Architecture","text":"<p>Backup is complicated and normally requires the use of specialized software. Backup client software sits on a system which has access to data sets for backup and is responsible for forwarding data to the backup server, which has access to the backing storage to be used for backup and recovery. There are very many different arrangements of server and backing store/media server where the backup data will be kept. There must also be a repository of meta-data about the backups; what was backed up, when, on what media, this may also originate with the client but will be stored at the server.</p> <ul> <li>Backing stores may be of many different types.</li> <li>Tape has been used for +50 years, it is linear and sequential. It is still the cheapest way to store large capacities over long time periods.</li> <li>Disk may be used in tape emulation mode and is now the most normal way to do initial backup.</li> <li>Disks can be used in an array for performance.</li> <li>With customized storage systems we can save space and time with deduplication</li> <li>Cloud storage, such as Amazon Glacier or Wasabi</li> </ul> <p>Backups are normally scheduled and for typical office applications, will be run after closing hours.</p> <p>An SME might backup directly to tape after hours. For larger operations, backup will be to disk during a backup window and to tape or cloud at some point afterwards.</p> <p>Systems which are always on are much more complex to plan. A partition might be snapshotted to allow for a stable copy to be taken. It could be mirrored to a different location and the backup taken of the mirror.</p> <p>In the event of a physical incident like a fire, there is little point looking for a backup in the adjacent cabinet. Remote backup can be performed site-to-site and this reduces the risk of simultaneously losing both the system and its backups. Alternatively, where the customer has only one site, we install the backup system as far away as possible. In the University, Letterkenny Campus, backup to disk occurs at the CO-LAB, in the far south east corner of the site, over 300m away from the data centres it provides backup for.</p> <p>For an SME, backup tapes should be kept in a proper fireproof media safe off site. Many business owners install the media safe at home.</p>"},{"location":"d/","title":"Location of Data","text":"<p>Historically, data was everywhere, on laptops, PDAs, tablets, PCs and servers. Modern practices and data protection regulations should be minimizing this. We need to apply some basic rules.</p> <p>If we are holding data on a mobile device, it needs to be restricted to fixed locations on that device. Data in these locations must be encrypted and protected and as part of this protection, must be backup up. The normal procedure currently is to synchronize data with either domain file servers or with the cloud. A lost mobile device must not result in either data compromise or data loss. </p> <p>Where data is located on servers, it must be in structured, well defined locations, with rules. I do not believe current practices are compliant with emerging legislative requirements; personally, I think the entire concept of file shares for any data that is subject to Data Protection (DP) is dead. Data needs to be stored with rich meta-data and although this is technically possible in file shares, its very difficult to implement and police. In any greenfield site, I would implement a document management system, at a minimum, something like Sharepoint.</p>"},{"location":"e/","title":"Backup Granularity","text":"<p>Once we know what our requirements are in terms of RPO and RTO, we can decide on the granularity of our backups. A starting point for any data set is to ask:</p> <ul> <li>How much data do we have?</li> <li>How much new/changed data is there per hour/day/week?</li> </ul> <p>Full backups are a copy of a data set and are the easiest to take, the easiest to track and to recover lost data from. However, they are expensive in capacity and time taken as we are backing up all data, including that which has not changed recently.</p> <p>More economical are incremental backups; these are backups of files which have changed since the last backup. This will be most complex to track, but it will be the most economical solution in terms of capacity and time taken. Restoring from an incremental backup requires access to the last full backup, plus all the incremental backups which have taken place since.</p> <p>A compromise between the two strategies above are differential or cumulative backups. These will be a backup of all files which have changed since the last full backup. Differential backups are less economical in storage space and time than incremental backups but are quicker to restore from and easy to track. Restoring from a differential backup requires access to the last full backup, and the most recent differential backup.</p> Fig 1. Spreadsheet Model. <p>A few minutes builds a simple spreadsheet for modelling this. The graph above shows requirements where the initial data capacity is 5GB and we are adding 1GB a day. The numbers were kept close together for presentation only. The advantage of a spreadsheet is that I can plug in numbers for a customer and get results, it makes scenario planning easier.</p> <p>Modern systems also allow the concept of a synthetic backup, where incremental backups are used to update an off-line copy of a full backup, such that a single full backup is available. This makes restores quick and easy.</p>"},{"location":"f/","title":"Disaster Recovery","text":"<p>Disaster Recovery or DR is required when a major incident creates a requirement to recreate the infrastructure and data of a site. In Ireland, this will be after a fire or a flood. In other countries, it may be required after a tsunami, earthquake, volcano, military, or paramilitary incident. There is an entire topic to be looked at here called Business Continuity Management or BCM. Some backups must be kept off-site to facilitate DR and the requirements here may be complex. In modern systems, we will try to have off-site replication, such that a failure on the main site may cause zero downtime. Large companies will have requirements for DR; for example, no DR site can be within 100kms of the primary site. Small sites may have a simpler system, where the weekly tapes go offsite to a data safe.</p> <p>In disaster recovery planning, we look at scenarios where our main site may not longer be accessible or even in existence. Think about Fukashima, Chernobyl, the tsunami event of 2004, etc.</p> <p>A cold site is a backup location which is not ready to go and may be in one of several states. - It may have no equipment. - It may have limited equipment. - It may have equipment in a powered down or unconfigured state. </p> <p>A hot site is a site that is ready to go. It has equipment fully operational and just needs a copy of current backups, VMs etc. to go live. A dedicated hot site is a very expensive contingency.  </p>"},{"location":"g/","title":"Operational backup and restore.","text":"<p>Operational backup and restore is the most common backup/restore activity. During the ordinary course of business, a requirement exists to restore data to a point in time, due to accidental deletion, or corruption, the requirement for a roll-back, or some other happenstance event. Restores are most commonly of files or whole directories. Occasionally there may be a requirement for database restores. It may be that training is required to minimize the need for restores.</p> <p>Backup windows need to be identified, where file systems and databases can be locked for backup; for example, in the Institute, from 2200-0700. If a system is live when we attempt the backup, we call that a hot backup. For systems which are continuously live, special agents are required to make consistent backups. Databases may be particularly difficult, as they consist of many files. In the case of either file systems or databases everything must be consistent in a backup, to make the backup usable.</p> <p>If we can shut down an application or system before backing it up, this is a cold backup and is normally preferred for data consistency and performance. When backing up file system data, it may be impossible for the backup system to get a file lock if the file is open, these files are locked by the underlying OS. Sometimes client software may be able to interact with the OS to make a copy of locked files for backup.</p>"},{"location":"h/","title":"Long term backups","text":"<p>Long term backups may be required for compliance or regulatory reasons. Most businesses in Ireland will keep accounting records for a fixed period based on data retention policies. The Institute will keep certain academic records for a period for similar reasons; it is important that the data retention period is known and complied with.</p> <p>Each system will have an associated dataset and policies may be different for each. For example, the payroll system may only require backups once per month if the payroll is only run once a month. However, we may need to keep the records indefinitely, due to superannuation and pension requirements. A t the expiration date, media must be securely erased. Although most standards say this should be done, the detail of how to do it is left to the user.</p>"},{"location":"i/","title":"Bare-metal recovery","text":"<p>Backup of application servers and workstations, sometimes called bare-metal recovery, is a hard issue to resolve. Building a server may be a complex job, taking many days and specialist skills. Bespoke software and configuration may have been performed, there may be unique licenses and keys. Whenever we audit a site, we find these little snowflakes, unique servers and workstations which no one knows ho configured or when and no one has the information to rebuild. </p> <p>Identifying these problem devices is the first step, but properly making these devices recoverable is a difficult problem to solve. One temporary mitigation is to perform a P2V, a physical to virtual conversion where we copy the contents of a physical machine to a VM. In most modern applications, we build the servers as VMs, backup and recovery become a very easy task. </p> <p>Procedures are important. </p> <p>One way of avoiding this problem is to ensure a server build record is created for each critical server. In an SME, this would be a file/box with instructions and copies of the media used. In a data centre, it\u2019s the build documentation or scripts for a VM or class of VMs.</p> <p>And its why I always want a build script as part of a systems documentation!</p>"},{"location":"j/","title":"SME Strategy","text":"<p>To finish I want to suggest a simple strategy for an SME. </p>"},{"location":"j/#business-continuity","title":"Business Continuity","text":"<p>For business continuity I want to design a system where if a server fails, we can just keep on working on the same site. I'm going to presume that you've been able to consolidate services onto a single host server.</p> <p>To begin, we set up a second identical host server. We split the critical services amongst the two hosts.</p> Service Host1 Host2 AAA dc1 dc2 DNS dc1 dc2 DHCP dc1 dc2 File Sharing fs1 fs2 <p>Active directory synchronizes automatically and if you set the DCs up correctly, so does DNS and DHCP.</p> <p>We can set the file servers up to synchronize using DFS and similar technologies (look it up!).</p> <p>We could still lose an entire VM! So I will write a script such that each VM shuts down during the night and makes a copy of itself to the other host.</p>"},{"location":"j/#backups","title":"Backups","text":"<p>I will supply a tape backup system (or two!) for each host. I need to design a backup strategy. I will also do a backup on the VMs, at a certain frequency.</p>"},{"location":"j/#disaster-recovery","title":"Disaster Recovery","text":"<p>Some of my tapes will go off site to a data safe. I will have a spare tape drive at that location, and an inexpensive server (Host 3). I will periodically test restores to this system.</p>"},{"location":"j/#making-it-work","title":"Making it work","text":"<p>It must be someones job to make sure this all works, define quality assurance for this, regular test!</p>"},{"location":"k/","title":"Finally","text":"<p>I am very critical of most standards bodies. In many cases they appear like a paper mill over necessary waffle! Some of the relevant standards for this section include;</p> Standard / Framework Focus Area ISO/IEC 27001 Secure backup &amp; recovery controls ISO 22301 Business continuity &amp; resilience NIST SP 800-34 Disaster recovery planning NIST SP 800-53 Security &amp; recovery controls ITIL IT service continuity COBIT Governance &amp; control objectives"}]}